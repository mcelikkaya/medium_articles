{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collaborative Filtering is usually with Matrix Factorization.\n",
    "\n",
    "#How to model Recommendation as a Neural Network?\n",
    "\n",
    "#A neural network is for modeling complex interactions between entities.\n",
    "#In our problem we have 2 entity.User and Jester.\n",
    "\n",
    "#How can we vectorize user ?\n",
    "#Ratings matrix is a good data for modeling a user by his likes.\n",
    "#So we have a user vector of 100 items.\n",
    "\n",
    "#***Here not reviewed jesters could lead some miss leading results.\n",
    "#But we can also think these as Dropout. So even our model can benefit from this.\n",
    "#I tried this approach also,it had good results but \n",
    "#then i decided to replace missing values with average of each user to get a embedding.\n",
    "#this gave better results. But I also think this add some bias towards mean of users.\n",
    "#We can exclude some items from set and try testing with some rated items excluded from training set.\n",
    "\n",
    "\n",
    "#How can we model a jester?\n",
    "\n",
    "#In fact that are latent features we don't know for Jester.\n",
    "#Example Jester could be political, or about marriage,daily life...\n",
    "#So we must find a way to vectorize Jester.\n",
    "\n",
    "#Rating is 24933 user by 100 Jester matrix.\n",
    "#If I transpose this then it becomes\n",
    "#100 jester by 24933 user.\n",
    "#This becomes an encoding for Jester based on user like.\n",
    "\n",
    "#In fact we can use this, but dimension is so high.\n",
    "#I tried 2 approaches, \n",
    "#Encode jesters by cosine, \n",
    "#Then I have a 100x100 matrix of Jester.This matrix shows how similar that one jester to others.\n",
    "\n",
    "#Encode jesters by user , i chosed 50 dimension for PCA,Isomap\n",
    "#Isomap a nonlinear dimension reduction so instead of having a jester\n",
    "#mapping as 24933 dimension, we will just have 50 dimension.\n",
    "#Then I have a 100x50 matrix of Jester.This matrix shows an encoding based on user like\n",
    "\n",
    "#I trained both models, both are good, Isomapis better, since it is exactly encoding\n",
    "#of what we want.\n",
    "\n",
    "#Conclusion :\n",
    "#I have a val_mean_absolute_error: 0.2 \n",
    "#My values range from -1 to 1.\n",
    "#so my error is 100 x 0.2 / 2 = 10\n",
    "#So i can claim to guess what rating will user give to a jester with 90 accuracy.\n",
    "#my guess will be around actual value by %10 ratio \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlrd\n",
    "#!pip install opt_einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import keras\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, concatenate, multiply, Input\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jester1</th>\n",
       "      <th>Jester2</th>\n",
       "      <th>Jester3</th>\n",
       "      <th>Jester4</th>\n",
       "      <th>Jester5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jane</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnold</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Jester1  Jester2  Jester3  Jester4  Jester5\n",
       "john        0.1      0.7      0.1      0.7      0.9\n",
       "jane        0.2      0.6      0.2      0.6      0.8\n",
       "arnold      0.7      0.9      0.1      0.3      0.1\n",
       "sam         0.6      0.7      0.1      0.5      0.1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features = [\"Office\",\"School\",\"Relations\",\"Political\",\"Sport\"]\n",
    "features = [\"Jester1\",\"Jester2\",\"Jester3\",\"Jester4\",\"Jester5\"]\n",
    "john = [0.1,0.7,0.1,0.7,0.9]\n",
    "jane  = [0.2,0.6,0.2,0.6,0.8]\n",
    "arnold  = [0.7,0.9,0.1,0.3,0.1]\n",
    "sam  = [0.6,0.7,0.1,0.5,0.1]\n",
    "\n",
    "\n",
    "classes = [\"john\",\"jane\",\"arnold\",\"sam\"]\n",
    "datas = np.stack([john,jane,arnold,sam])\n",
    "df_movie = pd.DataFrame(datas,columns=features,index=classes)\n",
    "df_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>john</th>\n",
       "      <th>jane</th>\n",
       "      <th>arnold</th>\n",
       "      <th>sam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jester1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jester2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jester3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jester4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jester5</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         john  jane  arnold  sam\n",
       "Jester1   0.1   0.2     0.7  0.6\n",
       "Jester2   0.7   0.6     0.9  0.7\n",
       "Jester3   0.1   0.2     0.1  0.1\n",
       "Jester4   0.7   0.6     0.3  0.5\n",
       "Jester5   0.9   0.8     0.1  0.1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of people: 24983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.782</td>\n",
       "      <td>0.879</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.786</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.184</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3      4      5      6      7      8      9      10   ...  \\\n",
       "0 -0.782  0.879 -0.966 -0.816 -0.752 -0.850 -0.985  0.417 -0.898 -0.476  ...   \n",
       "1  0.408 -0.029  0.636  0.437 -0.238 -0.966 -0.073 -0.534  0.888  0.922  ...   \n",
       "2    NaN    NaN    NaN    NaN  0.903  0.927  0.903  0.927    NaN    NaN  ...   \n",
       "3    NaN  0.835    NaN    NaN  0.180  0.816 -0.282  0.621    NaN  0.184  ...   \n",
       "4  0.850  0.461 -0.417 -0.539  0.136  0.160  0.704  0.461 -0.044  0.573  ...   \n",
       "\n",
       "     91     92     93     94     95     96     97     98     99     100  \n",
       "0  0.282    NaN    NaN    NaN    NaN    NaN -0.563    NaN    NaN    NaN  \n",
       "1  0.282 -0.495 -0.029  0.786 -0.019 -0.214  0.306  0.034 -0.432  0.107  \n",
       "2    NaN    NaN    NaN  0.908    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "3    NaN    NaN    NaN  0.053    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "4  0.519  0.558  0.427  0.519  0.573  0.155  0.311  0.655  0.180  0.160  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the whole data \n",
    "#so i normalize data from range [-10,10] to range [-1,1] \n",
    "def normalize_by10(x):\n",
    "    return x / 10\n",
    "\n",
    "def denormalize_by10(x):\n",
    "    return x * 10\n",
    "\n",
    "#read excel\n",
    "df_orig = pd.read_excel(\"D:/mldata/jester/jester_dataset_1_1/jester-data-1.xls\",index_col=None, header=None)\n",
    "\n",
    "#drop 1st column which is for how many items rated by user\n",
    "#replace 99 with None for easy manupulation\n",
    "#normalize data by dividing to 10 so i have -1 to 1 distribution.\n",
    "#data distribution seems nice,so i don't touch, i could try normalize by z value , \n",
    "df_normalized = df_orig.drop([0], 1).applymap(lambda x: None if x == 99 else x ).applymap(normalize_by10)\n",
    "print(\"number of people:\",len(df_normalized))\n",
    "#display sample\n",
    "df_normalized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-10: 74902,\n",
       " -9: 56355,\n",
       " -8: 58511,\n",
       " -7: 65006,\n",
       " -6: 53389,\n",
       " -5: 71202,\n",
       " -4: 72782,\n",
       " -3: 68934,\n",
       " -2: 77381,\n",
       " -1: 129495,\n",
       " 0: 116022,\n",
       " 1: 125567,\n",
       " 2: 127819,\n",
       " 3: 133486,\n",
       " 4: 111887,\n",
       " 5: 118113,\n",
       " 6: 109702,\n",
       " 7: 96957,\n",
       " 8: 104891,\n",
       " 9: 38052,\n",
       " 10: 2,\n",
       " 99: 687845}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check counts of ratings ( just round to )\n",
    "unique, counts = np.unique(df_normalized.drop([0], 1).applymap(lambda x: math.floor(x)), return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check counts of ratings ( just round to )\n",
    "unique, counts = np.unique(df_orig.drop([0], 1).applymap(lambda x: math.floor(x)), return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets discover distribution of data\n",
    "counts, bins = np.histogram( df_orig.drop([0], 1).values, bins='auto')\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24983, 100)\n",
      "(100, 24983)\n"
     ]
    }
   ],
   "source": [
    "#i will encode users by jester and jester by users\n",
    "df_user_jester = df_normalized\n",
    "df_jester_user = df_user_jester.T\n",
    "print( \"df_user_jester shape :\",df_user_jester.shape )\n",
    "print( \"df_jester_user shape :\",df_jester_user.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jester_user_imputed shape : (100, 24983)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24973</th>\n",
       "      <th>24974</th>\n",
       "      <th>24975</th>\n",
       "      <th>24976</th>\n",
       "      <th>24977</th>\n",
       "      <th>24978</th>\n",
       "      <th>24979</th>\n",
       "      <th>24980</th>\n",
       "      <th>24981</th>\n",
       "      <th>24982</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.782</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.090457</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.879</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942000</td>\n",
       "      <td>-0.922000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.917</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820000</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>-0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.816</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>0.301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.908</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>-0.144911</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.752</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.854000</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.777000</td>\n",
       "      <td>-0.971000</td>\n",
       "      <td>-0.228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24983 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1         2         3      4      5         6      7      8      \\\n",
       "1 -0.782  0.408  0.090457  0.090457  0.850 -0.617  0.090457  0.684 -0.379   \n",
       "2  0.879 -0.029  0.020846  0.835000  0.461 -0.354  0.020846  0.316 -0.354   \n",
       "3 -0.966  0.636  0.031657  0.031657 -0.417  0.044  0.031657  0.917 -0.942   \n",
       "4 -0.816  0.437 -0.144911 -0.144911 -0.539 -0.850 -0.144911 -0.621 -0.689   \n",
       "5 -0.752 -0.238  0.903000  0.180000  0.136 -0.709  0.859000 -0.816 -0.874   \n",
       "\n",
       "   9      ...     24973     24974     24975  24976     24977  24978  24979  \\\n",
       "1  0.301  ...  0.180000  0.090457  0.090457  0.136  0.090457  0.044  0.913   \n",
       "2  0.515  ... -0.942000 -0.922000  0.820000  0.175  0.020846  0.743 -0.816   \n",
       "3  0.515  ... -0.820000  0.031657  0.031657  0.238  0.031657  0.908  0.859   \n",
       "4  0.301  ... -0.144911 -0.144911 -0.144911  0.184 -0.144911  0.233  0.908   \n",
       "5  0.641  ... -0.854000  0.597000  0.879000  0.466  0.767000  0.320  0.087   \n",
       "\n",
       "      24980     24981  24982  \n",
       "1  0.090457  0.090457  0.243  \n",
       "2  0.020846  0.020846  0.267  \n",
       "3  0.031657  0.031657 -0.398  \n",
       "4 -0.144911 -0.144911  0.427  \n",
       "5 -0.777000 -0.971000 -0.228  \n",
       "\n",
       "[5 rows x 24983 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace not commented 99(formerly replaced as NaN) with average of row\n",
    "#average of row is, user's average like for jester set(of course not totally, it depends on which jester user rated )\n",
    "df_jester_user_imputed =  df_jester_user.T.fillna(df_jester_user.mean(axis=1)).T\n",
    "print( \"df_jester_user_imputed shape :\",df_jester_user_imputed.shape )\n",
    "df_jester_user_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_user_jester_imputed shape : (24983, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.782000</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>-0.966000</td>\n",
       "      <td>-0.816000</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.898000</td>\n",
       "      <td>-0.476000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.563000</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>-0.343189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408000</td>\n",
       "      <td>-0.029000</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>-0.495000</td>\n",
       "      <td>-0.029000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>-0.019000</td>\n",
       "      <td>-0.214000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>-0.432000</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.709939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.265812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>-0.417000</td>\n",
       "      <td>-0.539000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.044000</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1         2         3         4      5      6      7      8    \\\n",
       "0 -0.782000  0.879000 -0.966000 -0.816000 -0.752 -0.850 -0.985  0.417   \n",
       "1  0.408000 -0.029000  0.636000  0.437000 -0.238 -0.966 -0.073 -0.534   \n",
       "2  0.709939  0.709939  0.709939  0.709939  0.903  0.927  0.903  0.927   \n",
       "3  0.265812  0.835000  0.265812  0.265812  0.180  0.816 -0.282  0.621   \n",
       "4  0.850000  0.461000 -0.417000 -0.539000  0.136  0.160  0.704  0.461   \n",
       "\n",
       "        9         10   ...       91        92        93        94        95   \\\n",
       "0 -0.898000 -0.476000  ...  0.282000 -0.343189 -0.343189 -0.343189 -0.343189   \n",
       "1  0.888000  0.922000  ...  0.282000 -0.495000 -0.029000  0.786000 -0.019000   \n",
       "2  0.709939  0.709939  ...  0.709939  0.709939  0.709939  0.908000  0.709939   \n",
       "3  0.265812  0.184000  ...  0.265812  0.265812  0.265812  0.053000  0.265812   \n",
       "4 -0.044000  0.573000  ...  0.519000  0.558000  0.427000  0.519000  0.573000   \n",
       "\n",
       "        96        97        98        99        100  \n",
       "0 -0.343189 -0.563000 -0.343189 -0.343189 -0.343189  \n",
       "1 -0.214000  0.306000  0.034000 -0.432000  0.107000  \n",
       "2  0.709939  0.709939  0.709939  0.709939  0.709939  \n",
       "3  0.265812  0.265812  0.265812  0.265812  0.265812  \n",
       "4  0.155000  0.311000  0.655000  0.180000  0.160000  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for changing row missing values, i transpose ,fill with mean and then transpose again\n",
    "df_user_jester_imputed =  df_user_jester.T.fillna(df_user_jester.mean(axis=1)).T\n",
    "print( \"df_user_jester_imputed shape :\",df_user_jester_imputed.shape )\n",
    "df_user_jester_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jester_user_imputed shape (100, 24983)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a cosine embedding for jesters\n",
    "#this embedding is how similar each jester to other jesters\n",
    "print(\"df_jester_user_imputed shape\",df_jester_user_imputed.shape)\n",
    "cosine_similarities = cosine_similarity(df_jester_user_imputed, df_jester_user_imputed) \n",
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.362482</td>\n",
       "      <td>0.192402</td>\n",
       "      <td>0.154229</td>\n",
       "      <td>0.241124</td>\n",
       "      <td>0.141009</td>\n",
       "      <td>0.122613</td>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.343866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211532</td>\n",
       "      <td>0.166561</td>\n",
       "      <td>0.239209</td>\n",
       "      <td>0.219170</td>\n",
       "      <td>0.202124</td>\n",
       "      <td>0.217806</td>\n",
       "      <td>0.218745</td>\n",
       "      <td>0.165215</td>\n",
       "      <td>0.120304</td>\n",
       "      <td>0.211604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.281671</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.178298</td>\n",
       "      <td>0.211195</td>\n",
       "      <td>0.166878</td>\n",
       "      <td>0.098297</td>\n",
       "      <td>0.225424</td>\n",
       "      <td>0.257425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112402</td>\n",
       "      <td>0.108836</td>\n",
       "      <td>0.120372</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.109151</td>\n",
       "      <td>0.114361</td>\n",
       "      <td>0.126659</td>\n",
       "      <td>0.197690</td>\n",
       "      <td>0.104505</td>\n",
       "      <td>0.209245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.362482</td>\n",
       "      <td>0.281671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317075</td>\n",
       "      <td>0.181618</td>\n",
       "      <td>0.253264</td>\n",
       "      <td>0.140682</td>\n",
       "      <td>0.138792</td>\n",
       "      <td>0.293858</td>\n",
       "      <td>0.286815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143237</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>0.156439</td>\n",
       "      <td>0.145266</td>\n",
       "      <td>0.166194</td>\n",
       "      <td>0.141315</td>\n",
       "      <td>0.142453</td>\n",
       "      <td>0.167087</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.241857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.192402</td>\n",
       "      <td>0.268723</td>\n",
       "      <td>0.317075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130072</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.175679</td>\n",
       "      <td>0.111144</td>\n",
       "      <td>0.380116</td>\n",
       "      <td>0.119030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062334</td>\n",
       "      <td>0.059819</td>\n",
       "      <td>-0.081025</td>\n",
       "      <td>0.031028</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.010448</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.218569</td>\n",
       "      <td>0.139905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154229</td>\n",
       "      <td>0.178298</td>\n",
       "      <td>0.181618</td>\n",
       "      <td>0.130072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.187275</td>\n",
       "      <td>0.193046</td>\n",
       "      <td>0.150185</td>\n",
       "      <td>0.080237</td>\n",
       "      <td>0.162414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109825</td>\n",
       "      <td>0.094893</td>\n",
       "      <td>0.117665</td>\n",
       "      <td>0.094127</td>\n",
       "      <td>0.100794</td>\n",
       "      <td>0.094611</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>0.151462</td>\n",
       "      <td>0.094884</td>\n",
       "      <td>0.186813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.217806</td>\n",
       "      <td>0.114361</td>\n",
       "      <td>0.141315</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.094611</td>\n",
       "      <td>0.325146</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.230662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473786</td>\n",
       "      <td>0.456266</td>\n",
       "      <td>0.524742</td>\n",
       "      <td>0.479251</td>\n",
       "      <td>0.472858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.535115</td>\n",
       "      <td>0.289366</td>\n",
       "      <td>0.325406</td>\n",
       "      <td>0.349832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.218745</td>\n",
       "      <td>0.126659</td>\n",
       "      <td>0.142453</td>\n",
       "      <td>-0.010448</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>0.297368</td>\n",
       "      <td>0.064704</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>0.120687</td>\n",
       "      <td>0.220881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467301</td>\n",
       "      <td>0.393863</td>\n",
       "      <td>0.519620</td>\n",
       "      <td>0.446566</td>\n",
       "      <td>0.406739</td>\n",
       "      <td>0.535115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.306144</td>\n",
       "      <td>0.321826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.165215</td>\n",
       "      <td>0.197690</td>\n",
       "      <td>0.167087</td>\n",
       "      <td>0.171852</td>\n",
       "      <td>0.151462</td>\n",
       "      <td>0.231539</td>\n",
       "      <td>0.080746</td>\n",
       "      <td>-0.030296</td>\n",
       "      <td>0.083530</td>\n",
       "      <td>0.136954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276301</td>\n",
       "      <td>0.285830</td>\n",
       "      <td>0.325251</td>\n",
       "      <td>0.220889</td>\n",
       "      <td>0.245931</td>\n",
       "      <td>0.289366</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>0.450933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.120304</td>\n",
       "      <td>0.104505</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.218569</td>\n",
       "      <td>0.094884</td>\n",
       "      <td>0.179186</td>\n",
       "      <td>0.090675</td>\n",
       "      <td>0.065264</td>\n",
       "      <td>0.219272</td>\n",
       "      <td>0.118074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226313</td>\n",
       "      <td>0.289554</td>\n",
       "      <td>0.219427</td>\n",
       "      <td>0.302806</td>\n",
       "      <td>0.304224</td>\n",
       "      <td>0.325406</td>\n",
       "      <td>0.306144</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.209245</td>\n",
       "      <td>0.241857</td>\n",
       "      <td>0.139905</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>-0.006575</td>\n",
       "      <td>0.086629</td>\n",
       "      <td>0.201667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336834</td>\n",
       "      <td>0.307375</td>\n",
       "      <td>0.410524</td>\n",
       "      <td>0.257592</td>\n",
       "      <td>0.262982</td>\n",
       "      <td>0.349832</td>\n",
       "      <td>0.321826</td>\n",
       "      <td>0.450933</td>\n",
       "      <td>0.263642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.363639  0.362482  0.192402  0.154229  0.241124  0.141009   \n",
       "1   0.363639  1.000000  0.281671  0.268723  0.178298  0.211195  0.166878   \n",
       "2   0.362482  0.281671  1.000000  0.317075  0.181618  0.253264  0.140682   \n",
       "3   0.192402  0.268723  0.317075  1.000000  0.130072  0.140820  0.175679   \n",
       "4   0.154229  0.178298  0.181618  0.130072  1.000000  0.187275  0.193046   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.217806  0.114361  0.141315  0.012634  0.094611  0.325146  0.043766   \n",
       "96  0.218745  0.126659  0.142453 -0.010448  0.096928  0.297368  0.064704   \n",
       "97  0.165215  0.197690  0.167087  0.171852  0.151462  0.231539  0.080746   \n",
       "98  0.120304  0.104505  0.143311  0.218569  0.094884  0.179186  0.090675   \n",
       "99  0.211604  0.209245  0.241857  0.139905  0.186813  0.332005  0.061659   \n",
       "\n",
       "          7         8         9   ...        90        91        92        93  \\\n",
       "0   0.122613  0.280543  0.343866  ...  0.211532  0.166561  0.239209  0.219170   \n",
       "1   0.098297  0.225424  0.257425  ...  0.112402  0.108836  0.120372  0.107698   \n",
       "2   0.138792  0.293858  0.286815  ...  0.143237  0.141169  0.156439  0.145266   \n",
       "3   0.111144  0.380116  0.119030  ... -0.062334  0.059819 -0.081025  0.031028   \n",
       "4   0.150185  0.080237  0.162414  ...  0.109825  0.094893  0.117665  0.094127   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.009105  0.161700  0.230662  ...  0.473786  0.456266  0.524742  0.479251   \n",
       "96  0.012801  0.120687  0.220881  ...  0.467301  0.393863  0.519620  0.446566   \n",
       "97 -0.030296  0.083530  0.136954  ...  0.276301  0.285830  0.325251  0.220889   \n",
       "98  0.065264  0.219272  0.118074  ...  0.226313  0.289554  0.219427  0.302806   \n",
       "99 -0.006575  0.086629  0.201667  ...  0.336834  0.307375  0.410524  0.257592   \n",
       "\n",
       "          94        95        96        97        98        99  \n",
       "0   0.202124  0.217806  0.218745  0.165215  0.120304  0.211604  \n",
       "1   0.109151  0.114361  0.126659  0.197690  0.104505  0.209245  \n",
       "2   0.166194  0.141315  0.142453  0.167087  0.143311  0.241857  \n",
       "3   0.065239  0.012634 -0.010448  0.171852  0.218569  0.139905  \n",
       "4   0.100794  0.094611  0.096928  0.151462  0.094884  0.186813  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.472858  1.000000  0.535115  0.289366  0.325406  0.349832  \n",
       "96  0.406739  0.535115  1.000000  0.298900  0.306144  0.321826  \n",
       "97  0.245931  0.289366  0.298900  1.000000  0.275670  0.450933  \n",
       "98  0.304224  0.325406  0.306144  0.275670  1.000000  0.263642  \n",
       "99  0.262982  0.349832  0.321826  0.450933  0.263642  1.000000  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is a similarity matrix, check diagonals are 1\n",
    "#everything is totally similar to itself.\n",
    "pd.DataFrame(cosine_similarities).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data is 25K rows x 100 colums\n",
    "#for neural network i will use a \n",
    "#jester embeding + user embeding -> rating\n",
    "#so i create a row for every cell\n",
    "#this method return 25K x 100 = 2500K rows = \n",
    "def get_as_train_data(df_tabular,jester_encoding):    \n",
    "    num_rows = len(df_tabular)    \n",
    "    df_user_jester_values = df_tabular.values\n",
    "    user_joke_rating = []\n",
    "    y_rating = []\n",
    "    for index_user in range(num_rows):    \n",
    "        for joke_index,rating  in enumerate(df_tabular.iloc[index_user].values):        \n",
    "            #if cell defined use it if not skip\n",
    "            if not pd.isna( rating ):\n",
    "                    \n",
    "                jester_embedded = jester_encoding[joke_index]\n",
    "                user_embedded  = df_user_jester_values[index_user:index_user+1,:].flatten()            \n",
    "                                        \n",
    "                embed_concat = np.hstack( (jester_embedded,user_embedded)).flatten()            \n",
    "                y_rating.append(rating)                \n",
    "                user_joke_rating.append(embed_concat)\n",
    "                    \n",
    "    return np.array(user_joke_rating),np.array(y_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (2498300, 200)\n",
      "y_train (2498300,)\n"
     ]
    }
   ],
   "source": [
    "#get a training set for cosine trial\n",
    "x_train,y_train = get_as_train_data(df_user_jester_imputed,cosine_similarities)\n",
    "print(\"x_train\",x_train.shape)\n",
    "print(\"y_train\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.782,  0.879, -0.816, -0.752, -0.85 , -0.985,  0.417, -0.898,\n",
       "       -0.476, -0.85 ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "#I tried with different optimizations,different learning rates.\n",
    "#below configurations seems good enough, it could try schedulers also\n",
    "def mlp_model_cosine():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(120, input_dim=200, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(60, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "    \n",
    "\treturn model\n",
    "\n",
    "def mlp_model_iso_deep():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(160, input_dim=150, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(80, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(40, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\topt = keras.optimizers.Adam(lr=0.0001)\n",
    "\tmodel.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.782"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 120)               24120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 31,441\n",
      "Trainable params: 31,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define cosine model and train,\n",
    "# here we have train data as (2498300, 200) \n",
    "# 100 column jester embedding + 100 column user embedding\n",
    "#use early stopping\n",
    "model_cosine = mlp_model_cosine()\n",
    "\n",
    "model_cosine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainx (2498300, 200)\n",
      "trainy (2498300,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 120)               24120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 31,441\n",
      "Trainable params: 31,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2186012 samples, validate on 312288 samples\n",
      "Epoch 1/50\n",
      "2186012/2186012 [==============================] - 22s 10us/step - loss: 0.1355 - mean_absolute_error: 0.2686 - val_loss: 0.1378 - val_mean_absolute_error: 0.2682\n",
      "Epoch 2/50\n",
      "2186012/2186012 [==============================] - 24s 11us/step - loss: 0.1274 - mean_absolute_error: 0.2550 - val_loss: 0.1330 - val_mean_absolute_error: 0.2587\n",
      "Epoch 3/50\n",
      "2186012/2186012 [==============================] - 23s 10us/step - loss: 0.1233 - mean_absolute_error: 0.2484 - val_loss: 0.1286 - val_mean_absolute_error: 0.2534\n",
      "Epoch 4/50\n",
      "2186012/2186012 [==============================] - 24s 11us/step - loss: 0.1191 - mean_absolute_error: 0.2441 - val_loss: 0.1241 - val_mean_absolute_error: 0.2493\n",
      "Epoch 5/50\n",
      "2186012/2186012 [==============================] - 23s 11us/step - loss: 0.1149 - mean_absolute_error: 0.2400 - val_loss: 0.1198 - val_mean_absolute_error: 0.2449\n",
      "Epoch 6/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.1109 - mean_absolute_error: 0.2359 - val_loss: 0.1164 - val_mean_absolute_error: 0.2429\n",
      "Epoch 7/50\n",
      "2186012/2186012 [==============================] - 24s 11us/step - loss: 0.1078 - mean_absolute_error: 0.2324 - val_loss: 0.1133 - val_mean_absolute_error: 0.2391\n",
      "Epoch 8/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.1052 - mean_absolute_error: 0.2293 - val_loss: 0.1110 - val_mean_absolute_error: 0.2356\n",
      "Epoch 9/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.1030 - mean_absolute_error: 0.2264 - val_loss: 0.1086 - val_mean_absolute_error: 0.2321\n",
      "Epoch 10/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.1006 - mean_absolute_error: 0.2235 - val_loss: 0.1056 - val_mean_absolute_error: 0.2280\n",
      "Epoch 11/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.0981 - mean_absolute_error: 0.2206 - val_loss: 0.1036 - val_mean_absolute_error: 0.2260\n",
      "Epoch 12/50\n",
      "2186012/2186012 [==============================] - 27s 12us/step - loss: 0.0954 - mean_absolute_error: 0.2176 - val_loss: 0.1001 - val_mean_absolute_error: 0.2214\n",
      "Epoch 13/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0927 - mean_absolute_error: 0.2144 - val_loss: 0.0970 - val_mean_absolute_error: 0.2177\n",
      "Epoch 14/50\n",
      "2186012/2186012 [==============================] - 27s 12us/step - loss: 0.0899 - mean_absolute_error: 0.2110 - val_loss: 0.0941 - val_mean_absolute_error: 0.2148\n",
      "Epoch 15/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.0870 - mean_absolute_error: 0.2075 - val_loss: 0.0912 - val_mean_absolute_error: 0.2128\n",
      "Epoch 16/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.0841 - mean_absolute_error: 0.2039 - val_loss: 0.0881 - val_mean_absolute_error: 0.2076\n",
      "Epoch 17/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.0812 - mean_absolute_error: 0.2003 - val_loss: 0.0864 - val_mean_absolute_error: 0.2056\n",
      "Epoch 18/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.0784 - mean_absolute_error: 0.1967 - val_loss: 0.0821 - val_mean_absolute_error: 0.1989\n",
      "Epoch 19/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.0756 - mean_absolute_error: 0.1930 - val_loss: 0.0795 - val_mean_absolute_error: 0.1960\n",
      "Epoch 20/50\n",
      "2186012/2186012 [==============================] - 28s 13us/step - loss: 0.0728 - mean_absolute_error: 0.1895 - val_loss: 0.0761 - val_mean_absolute_error: 0.1925\n",
      "Epoch 21/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.0701 - mean_absolute_error: 0.1859 - val_loss: 0.0744 - val_mean_absolute_error: 0.1930\n",
      "Epoch 22/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.0677 - mean_absolute_error: 0.1826 - val_loss: 0.0720 - val_mean_absolute_error: 0.1867\n",
      "Epoch 23/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.0652 - mean_absolute_error: 0.1790 - val_loss: 0.0678 - val_mean_absolute_error: 0.1807\n",
      "Epoch 24/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.0628 - mean_absolute_error: 0.1758 - val_loss: 0.0660 - val_mean_absolute_error: 0.1785\n",
      "Epoch 25/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.0604 - mean_absolute_error: 0.1724 - val_loss: 0.0645 - val_mean_absolute_error: 0.1768\n",
      "Epoch 26/50\n",
      "2186012/2186012 [==============================] - 32s 14us/step - loss: 0.0581 - mean_absolute_error: 0.1688 - val_loss: 0.0685 - val_mean_absolute_error: 0.1910\n",
      "Epoch 27/50\n",
      "2186012/2186012 [==============================] - 42s 19us/step - loss: 0.0558 - mean_absolute_error: 0.1655 - val_loss: 0.0589 - val_mean_absolute_error: 0.1698\n",
      "Epoch 28/50\n",
      "2186012/2186012 [==============================] - 33s 15us/step - loss: 0.0536 - mean_absolute_error: 0.1621 - val_loss: 0.0565 - val_mean_absolute_error: 0.1655\n",
      "Epoch 29/50\n",
      "2186012/2186012 [==============================] - 32s 15us/step - loss: 0.0514 - mean_absolute_error: 0.1588 - val_loss: 0.0538 - val_mean_absolute_error: 0.1613\n",
      "Epoch 30/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.0494 - mean_absolute_error: 0.1557 - val_loss: 0.0580 - val_mean_absolute_error: 0.1718\n",
      "Epoch 31/50\n",
      "2186012/2186012 [==============================] - 32s 15us/step - loss: 0.0475 - mean_absolute_error: 0.1526 - val_loss: 0.0502 - val_mean_absolute_error: 0.1542\n",
      "Epoch 32/50\n",
      "2186012/2186012 [==============================] - 31s 14us/step - loss: 0.0458 - mean_absolute_error: 0.1498 - val_loss: 0.0471 - val_mean_absolute_error: 0.1496\n",
      "Epoch 33/50\n",
      "2186012/2186012 [==============================] - 27s 12us/step - loss: 0.0440 - mean_absolute_error: 0.1466 - val_loss: 0.0471 - val_mean_absolute_error: 0.1508\n",
      "Epoch 34/50\n",
      "2186012/2186012 [==============================] - 29s 13us/step - loss: 0.0423 - mean_absolute_error: 0.1438 - val_loss: 0.0445 - val_mean_absolute_error: 0.1457\n",
      "Epoch 35/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0409 - mean_absolute_error: 0.1414 - val_loss: 0.0426 - val_mean_absolute_error: 0.1440\n",
      "Epoch 36/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0392 - mean_absolute_error: 0.1383 - val_loss: 0.0409 - val_mean_absolute_error: 0.1392\n",
      "Epoch 37/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0377 - mean_absolute_error: 0.1355 - val_loss: 0.0434 - val_mean_absolute_error: 0.1470\n",
      "Epoch 38/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0362 - mean_absolute_error: 0.1328 - val_loss: 0.0382 - val_mean_absolute_error: 0.1342\n",
      "Epoch 39/50\n",
      "2186012/2186012 [==============================] - 25s 11us/step - loss: 0.0348 - mean_absolute_error: 0.1299 - val_loss: 0.0364 - val_mean_absolute_error: 0.1324\n",
      "Epoch 40/50\n",
      "2186012/2186012 [==============================] - 25s 11us/step - loss: 0.0335 - mean_absolute_error: 0.1276 - val_loss: 0.0372 - val_mean_absolute_error: 0.1354\n",
      "Epoch 41/50\n",
      "2186012/2186012 [==============================] - 27s 12us/step - loss: 0.0324 - mean_absolute_error: 0.1254 - val_loss: 0.0358 - val_mean_absolute_error: 0.1322\n",
      "Epoch 42/50\n",
      "2186012/2186012 [==============================] - 26s 12us/step - loss: 0.0309 - mean_absolute_error: 0.1220 - val_loss: 0.0454 - val_mean_absolute_error: 0.1592\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2186012/2186012 [==============================] - 24s 11us/step - loss: 0.0299 - mean_absolute_error: 0.1199 - val_loss: 0.0368 - val_mean_absolute_error: 0.1385\n",
      "Epoch 44/50\n",
      "2186012/2186012 [==============================] - 24s 11us/step - loss: 0.0288 - mean_absolute_error: 0.1177 - val_loss: 0.0314 - val_mean_absolute_error: 0.1226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"trainx\",x_train.shape)\n",
    "print(\"trainy\",y_train.shape)\n",
    "print(model_cosine.summary())\n",
    "#if no improvement in 5 epoch more than 0.01 then quit training\n",
    "#we could also use Model checkpoint to save best model\n",
    "es = EarlyStopping(monitor='val_mean_absolute_error', mode='min', min_delta=0.01,patience=5)\n",
    "history = model_cosine.fit(x_train,y_train,batch_size = 256,\n",
    "                           shuffle=True,validation_split = 0.125,epochs = 50,verbose = 1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['mean_absolute_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_absolute_error'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "#seems validation begin fluctuating after 20-25 epoch,\n",
    "#since this model is simple and no regularization(like dropout) it begins overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is for preparing suggestiong by user\n",
    "#model_trained : model trained\n",
    "#user_index : which user we want to suggest for\n",
    "#user_embeddings : source of user_embedding, we have only 1 type of embedding\n",
    "#jester_embeddings : source of jester embedding, could be cosine or isomap\n",
    "#\n",
    "#return\n",
    "#predictions : what will model predict for this user (for all 100 jester)\n",
    "#df_all : just a  dataframe for checking results for all 100 jester\n",
    "#df_suggestions : a  dataframe for checking results for only not rated items\n",
    "#mean_abs_error : what will our model's absolute error be for this user\n",
    "def get_user_preds(model_trained,user_index,user_embeddings,jester_embeddings,is_return_suggestions):\n",
    "    #get user row values\n",
    "    user_row =  user_embeddings.iloc[user_index].values\n",
    "    #i want to also know Nan values\n",
    "    normalized_row = df_normalized.iloc[user_index].values\n",
    "    #create jester + user embeddings for predictions for all combinations    \n",
    "    all_predict_input = []\n",
    "    for i in range(len(jester_embeddings)):\n",
    "        jester_embedded = jester_embeddings[i]\n",
    "        embed_concat = np.hstack( (jester_embedded,user_row)).flatten()\n",
    "        all_predict_input.append( embed_concat )\n",
    "    #predict all movies for user(even ones which were rated before)    \n",
    "    predictions = model_trained.predict(np.array(all_predict_input))\n",
    "    \n",
    "    actual_prediction = []\n",
    "    abs_error = 0\n",
    "    num_notnulls = 0\n",
    "    status = []\n",
    "    for i in range(len(jester_embedded)):\n",
    "        #df_normalized has data with nan ,only add rated items to absolute error\n",
    "        if not pd.isna(  normalized_row[i] ):\n",
    "            status.append(\"Existing\")\n",
    "            num_notnulls += 1\n",
    "            abs_error += abs(user_row[i] - predictions[i][0])\n",
    "        else:\n",
    "            status.append(\"New\")\n",
    "                                            \n",
    "        actual_prediction.append([round(user_row[i],3) ,round(predictions[i][0],3) ])\n",
    "    #mean absolute error for user\n",
    "    mean_abs_error= abs_error / num_notnulls\n",
    "    \n",
    "    #I want to create a dataframe for predictions\n",
    "    #Actual : Value in original file\n",
    "    #Prediction : What will this network guess for this row\n",
    "    #stat : status of cell, New means it is guessed rating, Existing means already rated\n",
    "    df_all=pd.DataFrame(actual_prediction,columns=[\"actual\",\"prediction\"])\n",
    "    #before we divide values by 10 ,now multiple 10 \n",
    "    df_all = df_all.applymap(denormalize_by10)        \n",
    "    df_all[\"status\"] = status\n",
    "    #also return suggestions as a 2nd dataframe\n",
    "    df_suggestions = df_all[df_all[\"status\"]==\"New\"]\n",
    "    #sort suggestions\n",
    "    df_suggestions = df_suggestions.sort_values('prediction',ascending=False)\n",
    "    return predictions ,df_all ,df_suggestions,mean_abs_error\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_suggestions len 47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.76</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.33</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.28</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.13</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.76</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.53</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.43</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.39</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.26</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  prediction status\n",
       "77   -0.16        1.76    New\n",
       "94   -0.16        1.33    New\n",
       "95   -0.16        1.28    New\n",
       "99   -0.16        1.13    New\n",
       "86   -0.16        0.76    New\n",
       "2    -0.16        0.53    New\n",
       "50   -0.16        0.43    New\n",
       "87   -0.16        0.39    New\n",
       "57   -0.16        0.31    New\n",
       "96   -0.16        0.26    New\n",
       "56   -0.16        0.23    New\n",
       "80   -0.16        0.20    New\n",
       "98   -0.16        0.14    New\n",
       "73   -0.16        0.13    New\n",
       "97   -0.16        0.12    New\n",
       "88   -0.16        0.11    New\n",
       "75   -0.16        0.10    New\n",
       "40   -0.16        0.04    New\n",
       "23   -0.16        0.04    New\n",
       "29   -0.16       -0.04    New\n",
       "84   -0.16       -0.05    New\n",
       "83   -0.16       -0.14    New\n",
       "74   -0.16       -0.24    New\n",
       "91   -0.16       -0.38    New\n",
       "85   -0.16       -0.41    New\n",
       "89   -0.16       -0.42    New\n",
       "93   -0.16       -0.45    New\n",
       "92   -0.16       -0.47    New\n",
       "32   -0.16       -0.48    New\n",
       "90   -0.16       -0.52    New\n",
       "71   -0.16       -0.63    New\n",
       "79   -0.16       -0.72    New\n",
       "3    -0.16       -0.80    New\n",
       "8    -0.16       -0.87    New\n",
       "72   -0.16       -0.93    New\n",
       "82   -0.16       -0.98    New\n",
       "81   -0.16       -1.22    New\n",
       "70   -0.16       -1.31    New\n",
       "39   -0.16       -1.31    New\n",
       "76   -0.16       -1.46    New\n",
       "43   -0.16       -1.58    New\n",
       "63   -0.16       -1.63    New\n",
       "21   -0.16       -2.04    New\n",
       "36   -0.16       -2.19    New\n",
       "0    -0.16       -2.73    New\n",
       "66   -0.16       -3.04    New\n",
       "13   -0.16       -3.35    New"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions  ,df_all  ,df_suggestions,mean_abs_error= get_user_preds(model_cosine, 19,df_user_jester_imputed,cosine_similarities,True)\n",
    "#check suggestions for user\n",
    "#here actual is average of user infact it is Nan\n",
    "print(\"df_suggestions len\",len(df_suggestions))\n",
    "df_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_abs_error for this user 0.13\n",
      "100\n",
      "existing predictions : 53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.62</td>\n",
       "      <td>6.04</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.64</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.93</td>\n",
       "      <td>7.61</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-6.60</td>\n",
       "      <td>-6.29</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-9.47</td>\n",
       "      <td>-9.27</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9.27</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.02</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3.16</td>\n",
       "      <td>-3.62</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.69</td>\n",
       "      <td>2.88</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.81</td>\n",
       "      <td>-4.12</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.79</td>\n",
       "      <td>6.29</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-7.18</td>\n",
       "      <td>-6.01</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.62</td>\n",
       "      <td>4.41</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.77</td>\n",
       "      <td>0.48</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.25</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.44</td>\n",
       "      <td>5.79</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-9.95</td>\n",
       "      <td>-8.09</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.38</td>\n",
       "      <td>6.16</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.67</td>\n",
       "      <td>3.11</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-8.69</td>\n",
       "      <td>-5.57</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.00</td>\n",
       "      <td>3.86</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.22</td>\n",
       "      <td>5.77</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-9.27</td>\n",
       "      <td>-9.64</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-7.62</td>\n",
       "      <td>-8.60</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-8.83</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-9.42</td>\n",
       "      <td>-8.57</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-6.55</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-4.08</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2.40</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.41</td>\n",
       "      <td>4.04</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.09</td>\n",
       "      <td>1.19</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.37</td>\n",
       "      <td>8.19</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.46</td>\n",
       "      <td>0.68</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.12</td>\n",
       "      <td>3.14</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.28</td>\n",
       "      <td>1.82</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-6.50</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-1.12</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.98</td>\n",
       "      <td>5.01</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-2.48</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.94</td>\n",
       "      <td>0.04</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.20</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.31</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  prediction    status\n",
       "1     7.62        6.04  Existing\n",
       "4    -8.64       -8.68  Existing\n",
       "5     2.43        0.45  Existing\n",
       "6     8.93        7.61  Existing\n",
       "7    -6.60       -6.29  Existing\n",
       "9    -9.47       -9.27  Existing\n",
       "10   -2.77       -2.74  Existing\n",
       "11   -0.73        0.50  Existing\n",
       "12   -9.27       -9.31  Existing\n",
       "14   -1.02       -1.03  Existing\n",
       "15   -3.16       -3.62  Existing\n",
       "16    3.69        2.88  Existing\n",
       "17   -4.81       -4.12  Existing\n",
       "18    8.79        6.29  Existing\n",
       "19   -7.18       -6.01  Existing\n",
       "20    7.62        4.41  Existing\n",
       "22    2.77        0.48  Existing\n",
       "24    3.25       -0.18  Existing\n",
       "25    1.80        0.60  Existing\n",
       "26    5.44        5.79  Existing\n",
       "27   -9.95       -8.09  Existing\n",
       "28    7.38        6.16  Existing\n",
       "30    2.67        3.11  Existing\n",
       "31   -8.69       -5.57  Existing\n",
       "33   -0.58       -1.18  Existing\n",
       "34    5.00        3.86  Existing\n",
       "35    9.22        5.77  Existing\n",
       "37   -9.27       -9.64  Existing\n",
       "38   -7.62       -8.60  Existing\n",
       "41   -8.83       -4.24  Existing\n",
       "42   -9.42       -8.57  Existing\n",
       "44   -6.55       -6.18  Existing\n",
       "45   -4.08       -3.68  Existing\n",
       "46    2.86        2.40  Existing\n",
       "47    6.41        4.04  Existing\n",
       "48    2.09        1.19  Existing\n",
       "49    9.37        8.19  Existing\n",
       "51    1.46        0.68  Existing\n",
       "52    6.12        3.14  Existing\n",
       "53   -1.94       -1.87  Existing\n",
       "54    0.29       -0.44  Existing\n",
       "55    2.28        1.82  Existing\n",
       "58   -6.50       -3.90  Existing\n",
       "59   -1.12       -1.70  Existing\n",
       "60    4.81        0.73  Existing\n",
       "61    3.98        5.01  Existing\n",
       "62   -2.48       -3.13  Existing\n",
       "64    1.94        0.04  Existing\n",
       "65    0.63        0.20  Existing\n",
       "67    1.31       -1.61  Existing\n",
       "68    1.80        2.65  Existing\n",
       "69    0.29       -1.61  Existing\n",
       "78    0.00       -2.35  Existing"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check all values for user\n",
    "print(\"mean_abs_error for this user\",round(mean_abs_error,2))\n",
    "print(len(df_all))\n",
    "print(\"existing predictions :\",len(df_all[df_all[\"status\"] == \"Existing\"]))\n",
    "df_all[df_all[\"status\"] == \"Existing\"]\n",
    "#explanation of data for 1st row\n",
    "\n",
    "#actual prediction stat\n",
    "#3.01 4.09 Existing\n",
    "\n",
    "#originally user rated 3.01\n",
    "#if we had guessed the rating with our model we will guess as 4.09\n",
    "#difference is nearly 1\n",
    "#our values are between -10 and 10 \n",
    "#so 1 error in 20 \n",
    "# %5 error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_jester_user_imputed shape: (100, 24983)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_jester_user_imputed shape:\",df_jester_user_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a isomap dimension reduction,and use it for embedding\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Uncomment PCA to try with PCA\n",
    "embedding = Isomap(n_components=50)\n",
    "#embedding = PCA(n_components=50)\n",
    "isomap_jester_user = embedding.fit_transform(df_jester_user_imputed)\n",
    "isomap_jester_user.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.14369566, -53.4447873 ,  71.76056713, -16.45979903,\n",
       "         -9.95014713,  52.14496558,  56.57160855, -26.21157677,\n",
       "         11.2128202 , -11.29871776,  -6.44646712, -15.15314767,\n",
       "        -24.00819616,   7.1082318 , -29.42260822, -32.75670129,\n",
       "        -25.79705065, -29.75658148,   3.22306676,  11.29462771,\n",
       "         -4.97200716, -12.96189093,  10.19375667,  -3.59249533,\n",
       "         14.59007113, -24.91498061,  24.1835739 ,  18.15537926,\n",
       "         -4.53117301,  -9.36361389,   0.34956097,  -6.82847442,\n",
       "         17.4476063 ,  20.03144346,  -5.73676193, -14.00569047,\n",
       "         -0.38443106,   5.54708552,  -8.5311655 ,   7.70771376,\n",
       "        -16.29256152,  10.04040147,  -6.72577691,   0.65183937,\n",
       "         -3.59887713,  -4.08181378,  -3.78276877,   0.56247239,\n",
       "          3.76266875,   0.85011642]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check how it seems\n",
    "#if u want to see in better format open comment below\n",
    "#pd.DataFrame(isomap_jester_user).head(10)\n",
    "#how a row of encoding seems\n",
    "isomap_jester_user[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003598</td>\n",
       "      <td>-0.031299</td>\n",
       "      <td>0.042025</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>-0.006617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>-0.003939</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>-0.002108</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013318</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.061208</td>\n",
       "      <td>0.047045</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.038146</td>\n",
       "      <td>0.055198</td>\n",
       "      <td>0.012870</td>\n",
       "      <td>-0.013728</td>\n",
       "      <td>-0.016937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002434</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>-0.001686</td>\n",
       "      <td>-0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034264</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>0.041423</td>\n",
       "      <td>-0.022792</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>0.042684</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>-0.002349</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>-0.002408</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>-0.001955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.082741</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>-0.022482</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>-0.028907</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>-0.003608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023486</td>\n",
       "      <td>0.016514</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>0.060635</td>\n",
       "      <td>-0.001698</td>\n",
       "      <td>0.022671</td>\n",
       "      <td>-0.022397</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>-0.019012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004757</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>-0.002782</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.005726</td>\n",
       "      <td>-0.007892</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>0.002359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.039439</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>-0.026738</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>-0.000620</td>\n",
       "      <td>-0.019466</td>\n",
       "      <td>-0.009239</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>-0.033870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000751</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>-0.004297</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>-0.007593</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.049959</td>\n",
       "      <td>-0.016227</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.045663</td>\n",
       "      <td>-0.012821</td>\n",
       "      <td>-0.001726</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>-0.017210</td>\n",
       "      <td>0.042930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>-0.000675</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.056970</td>\n",
       "      <td>-0.043046</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.007771</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>0.011769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.001713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.084335</td>\n",
       "      <td>-0.004663</td>\n",
       "      <td>0.036704</td>\n",
       "      <td>-0.044544</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>-0.010944</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>-0.004444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>-0.008509</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>-0.006199</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001511</td>\n",
       "      <td>-0.059048</td>\n",
       "      <td>0.031583</td>\n",
       "      <td>-0.013235</td>\n",
       "      <td>-0.014597</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>0.023993</td>\n",
       "      <td>0.026323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>-0.010668</td>\n",
       "      <td>-0.005474</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>-0.001149</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>-0.004447</td>\n",
       "      <td>-0.002110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.003598 -0.031299  0.042025 -0.009639 -0.005827  0.030537  0.033130   \n",
       "1  0.013318  0.031538  0.061208  0.047045  0.000832  0.038146  0.055198   \n",
       "2  0.034264  0.004857  0.029562  0.021828  0.034461  0.041423 -0.022792   \n",
       "3  0.082741  0.013235 -0.022482  0.009294  0.008043  0.013322 -0.028907   \n",
       "4  0.023486  0.016514 -0.015396  0.060635 -0.001698  0.022671 -0.022397   \n",
       "5 -0.039439 -0.000256 -0.026738  0.023171 -0.000620 -0.019466 -0.009239   \n",
       "6  0.049959 -0.016227 -0.002842  0.045663 -0.012821 -0.001726  0.008146   \n",
       "7  0.056970 -0.043046  0.006895  0.007771  0.015078 -0.011905  0.023666   \n",
       "8  0.084335 -0.004663  0.036704 -0.044544  0.009813  0.012348 -0.010944   \n",
       "9  0.001511 -0.059048  0.031583 -0.013235 -0.014597  0.003822  0.004080   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0 -0.015350  0.006567 -0.006617  ... -0.009541  0.005880 -0.003939  0.000382   \n",
       "1  0.012870 -0.013728 -0.016937  ... -0.002434 -0.000904  0.001516 -0.001411   \n",
       "2 -0.021133  0.042684  0.008258  ...  0.002676  0.004587 -0.001496  0.002136   \n",
       "3 -0.030347  0.007491 -0.003608  ...  0.000034  0.001885  0.000874  0.005193   \n",
       "4  0.038251  0.007865 -0.019012  ... -0.004757 -0.004979 -0.002782  0.000146   \n",
       "5  0.006821  0.004153 -0.033870  ... -0.000751 -0.005325 -0.004297  0.005027   \n",
       "6 -0.024678 -0.017210  0.042930  ...  0.000974 -0.000675 -0.006745 -0.002923   \n",
       "7 -0.012072 -0.001327  0.011769  ...  0.005525  0.002174  0.000494  0.001839   \n",
       "8  0.021749 -0.007807 -0.004444  ...  0.008272  0.003088 -0.008509  0.001924   \n",
       "9 -0.004241  0.023993  0.026323  ...  0.003597 -0.010668 -0.005474  0.005340   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0 -0.002108 -0.002390 -0.002215  0.000329  0.002204  0.000498  \n",
       "1 -0.000201  0.001552  0.001985  0.001145 -0.001686 -0.001314  \n",
       "2 -0.002349  0.005103  0.000431 -0.002408  0.000762 -0.001955  \n",
       "3  0.004051  0.002528 -0.002681 -0.000200  0.002835  0.000041  \n",
       "4 -0.005726 -0.007892 -0.003019 -0.001621 -0.000625  0.002359  \n",
       "5 -0.007593  0.000213  0.000886 -0.000128 -0.003073  0.000143  \n",
       "6  0.005839 -0.000342 -0.000494  0.002035  0.002682  0.000033  \n",
       "7 -0.007990 -0.004937  0.000721  0.004870  0.000565  0.001713  \n",
       "8 -0.006199  0.002876  0.004734 -0.000151  0.002749  0.001214  \n",
       "9  0.000569  0.006516 -0.001149 -0.006269 -0.004447 -0.002110  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#values are not scaled, lets normalize them, so data becomes better distributed\n",
    "normalized_isomap_jester_user = isomap_jester_user / np.sqrt(np.sum(isomap_jester_user**2))\n",
    "pd.DataFrame(normalized_isomap_jester_user).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_user_jester_imputed shape: (24983, 100)\n",
      "normalized_isomap_jester_user shape: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_user_jester_imputed shape:\",df_user_jester_imputed.shape)\n",
    "print(\"normalized_isomap_jester_user shape:\",normalized_isomap_jester_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_iso (2498300, 150)\n",
      "y_train_iso (2498300,)\n"
     ]
    }
   ],
   "source": [
    "#get a new projected data set\n",
    "#here embedding length is 150\n",
    "#50 jester columns + 100 user columns\n",
    "x_train_iso,y_train_iso = get_as_train_data(df_user_jester_imputed,normalized_isomap_jester_user)\n",
    "print(\"x_train_iso\",x_train_iso.shape)\n",
    "print(\"y_train_iso\",y_train_iso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_76 (Dense)             (None, 160)               24160     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 80)                12880     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 40,321\n",
      "Trainable params: 40,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2186012 samples, validate on 312288 samples\n",
      "Epoch 1/50\n",
      "2186012/2186012 [==============================] - 38s 17us/step - loss: 0.0277 - mean_absolute_error: 0.3089 - val_loss: 0.0260 - val_mean_absolute_error: 0.3112\n",
      "Epoch 2/50\n",
      "2186012/2186012 [==============================] - 38s 18us/step - loss: 0.0254 - mean_absolute_error: 0.2995 - val_loss: 0.0253 - val_mean_absolute_error: 0.3062\n",
      "Epoch 3/50\n",
      "2186012/2186012 [==============================] - 40s 18us/step - loss: 0.0245 - mean_absolute_error: 0.2949 - val_loss: 0.0242 - val_mean_absolute_error: 0.3026\n",
      "Epoch 4/50\n",
      "2186012/2186012 [==============================] - 37s 17us/step - loss: 0.0234 - mean_absolute_error: 0.2899 - val_loss: 0.0229 - val_mean_absolute_error: 0.2977\n",
      "Epoch 5/50\n",
      "2186012/2186012 [==============================] - 37s 17us/step - loss: 0.0221 - mean_absolute_error: 0.2841 - val_loss: 0.0213 - val_mean_absolute_error: 0.2901\n",
      "Epoch 6/50\n",
      "2186012/2186012 [==============================] - 37s 17us/step - loss: 0.0204 - mean_absolute_error: 0.2767 - val_loss: 0.0191 - val_mean_absolute_error: 0.2827\n",
      "Epoch 7/50\n",
      "2186012/2186012 [==============================] - 38s 17us/step - loss: 0.0181 - mean_absolute_error: 0.2666 - val_loss: 0.0153 - val_mean_absolute_error: 0.2634\n",
      "Epoch 8/50\n",
      "2186012/2186012 [==============================] - 37s 17us/step - loss: 0.0153 - mean_absolute_error: 0.2529 - val_loss: 0.0115 - val_mean_absolute_error: 0.2448\n",
      "Epoch 9/50\n",
      "2186012/2186012 [==============================] - 38s 17us/step - loss: 0.0127 - mean_absolute_error: 0.2383 - val_loss: 0.0087 - val_mean_absolute_error: 0.2278\n",
      "Epoch 10/50\n",
      "2186012/2186012 [==============================] - 36s 16us/step - loss: 0.0112 - mean_absolute_error: 0.2283 - val_loss: 0.0072 - val_mean_absolute_error: 0.2175\n",
      "Epoch 11/50\n",
      "2186012/2186012 [==============================] - 36s 17us/step - loss: 0.0102 - mean_absolute_error: 0.2216 - val_loss: 0.0065 - val_mean_absolute_error: 0.2161\n",
      "Epoch 12/50\n",
      "2186012/2186012 [==============================] - 37s 17us/step - loss: 0.0095 - mean_absolute_error: 0.2158 - val_loss: 0.0058 - val_mean_absolute_error: 0.2077\n",
      "Epoch 13/50\n",
      "2186012/2186012 [==============================] - 41s 19us/step - loss: 0.0089 - mean_absolute_error: 0.2100 - val_loss: 0.0058 - val_mean_absolute_error: 0.2092\n",
      "Epoch 14/50\n",
      "2186012/2186012 [==============================] - 40s 18us/step - loss: 0.0085 - mean_absolute_error: 0.2056 - val_loss: 0.0055 - val_mean_absolute_error: 0.2086\n",
      "Epoch 15/50\n",
      "2186012/2186012 [==============================] - 50s 23us/step - loss: 0.0082 - mean_absolute_error: 0.2021 - val_loss: 0.0053 - val_mean_absolute_error: 0.2066\n",
      "Epoch 16/50\n",
      "2186012/2186012 [==============================] - 42s 19us/step - loss: 0.0079 - mean_absolute_error: 0.1983 - val_loss: 0.0053 - val_mean_absolute_error: 0.2040\n",
      "Epoch 17/50\n",
      "2186012/2186012 [==============================] - 49s 22us/step - loss: 0.0078 - mean_absolute_error: 0.1958 - val_loss: 0.0052 - val_mean_absolute_error: 0.2028\n",
      "Epoch 18/50\n",
      "2186012/2186012 [==============================] - 42s 19us/step - loss: 0.0077 - mean_absolute_error: 0.1938 - val_loss: 0.0049 - val_mean_absolute_error: 0.1988\n",
      "Epoch 19/50\n",
      "2186012/2186012 [==============================] - 50s 23us/step - loss: 0.0075 - mean_absolute_error: 0.1912 - val_loss: 0.0053 - val_mean_absolute_error: 0.1972\n",
      "Epoch 20/50\n",
      "2186012/2186012 [==============================] - 50s 23us/step - loss: 0.0074 - mean_absolute_error: 0.1890 - val_loss: 0.0048 - val_mean_absolute_error: 0.1945\n",
      "Epoch 21/50\n",
      "2186012/2186012 [==============================] - 46s 21us/step - loss: 0.0073 - mean_absolute_error: 0.1876 - val_loss: 0.0051 - val_mean_absolute_error: 0.1984\n",
      "Epoch 22/50\n",
      "2186012/2186012 [==============================] - 46s 21us/step - loss: 0.0072 - mean_absolute_error: 0.1850 - val_loss: 0.0052 - val_mean_absolute_error: 0.1935\n",
      "Epoch 23/50\n",
      "2186012/2186012 [==============================] - 47s 21us/step - loss: 0.0072 - mean_absolute_error: 0.1840 - val_loss: 0.0052 - val_mean_absolute_error: 0.1951\n",
      "Epoch 24/50\n",
      "2186012/2186012 [==============================] - 46s 21us/step - loss: 0.0071 - mean_absolute_error: 0.1823 - val_loss: 0.0051 - val_mean_absolute_error: 0.1908\n",
      "Epoch 25/50\n",
      "2186012/2186012 [==============================] - 50s 23us/step - loss: 0.0070 - mean_absolute_error: 0.1804 - val_loss: 0.0053 - val_mean_absolute_error: 0.1928\n"
     ]
    }
   ],
   "source": [
    "#get isomap model\n",
    "model_isomap_deep = mlp_model_iso_deep()\n",
    "print(model_isomap_deep.summary())\n",
    "\n",
    "#if no improvement of 0.01 on mean absolute error, for 5 epochs dont train anymore\n",
    "es = EarlyStopping(monitor='val_mean_absolute_error', mode='min', min_delta=0.01,patience=5)\n",
    "\n",
    "history_iso = model_isomap_deep.fit(x_train_iso,y_train_iso,\n",
    "                                    batch_size = 256,shuffle=True,\n",
    "                                    validation_split = 0.125,\n",
    "                                    epochs = 50,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history_iso.history['mean_absolute_error'], label='train')\n",
    "pyplot.plot(history_iso.history['val_mean_absolute_error'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_abs_error for this user 0.34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.62</td>\n",
       "      <td>5.97</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.64</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.43</td>\n",
       "      <td>1.70</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.93</td>\n",
       "      <td>7.12</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-6.60</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-9.47</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9.27</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.02</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.69</td>\n",
       "      <td>3.20</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.81</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.79</td>\n",
       "      <td>6.80</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-7.18</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.62</td>\n",
       "      <td>5.54</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.77</td>\n",
       "      <td>2.05</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.13</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.44</td>\n",
       "      <td>5.09</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-9.95</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.38</td>\n",
       "      <td>6.02</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.67</td>\n",
       "      <td>1.47</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-8.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.49</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.22</td>\n",
       "      <td>6.84</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-9.27</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-7.62</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-8.83</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-9.42</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-6.55</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-4.08</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.86</td>\n",
       "      <td>2.32</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.41</td>\n",
       "      <td>5.02</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.09</td>\n",
       "      <td>1.44</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.37</td>\n",
       "      <td>7.73</td>\n",
       "      <td>Existing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  prediction    status\n",
       "1     7.62        5.97  Existing\n",
       "4    -8.64       -1.53  Existing\n",
       "5     2.43        1.70  Existing\n",
       "6     8.93        7.12  Existing\n",
       "7    -6.60        0.03  Existing\n",
       "9    -9.47       -1.02  Existing\n",
       "10   -2.77        0.06  Existing\n",
       "11   -0.73       -1.08  Existing\n",
       "12   -9.27       -0.90  Existing\n",
       "14   -1.02       -1.76  Existing\n",
       "15   -3.16       -1.39  Existing\n",
       "16    3.69        3.20  Existing\n",
       "17   -4.81       -0.73  Existing\n",
       "18    8.79        6.80  Existing\n",
       "19   -7.18       -1.31  Existing\n",
       "20    7.62        5.54  Existing\n",
       "22    2.77        2.05  Existing\n",
       "24    3.25        1.98  Existing\n",
       "25    1.80        1.13  Existing\n",
       "26    5.44        5.09  Existing\n",
       "27   -9.95       -0.72  Existing\n",
       "28    7.38        6.02  Existing\n",
       "30    2.67        1.47  Existing\n",
       "31   -8.69        0.73  Existing\n",
       "33   -0.58        0.30  Existing\n",
       "34    5.00        4.49  Existing\n",
       "35    9.22        6.84  Existing\n",
       "37   -9.27       -1.14  Existing\n",
       "38   -7.62       -0.74  Existing\n",
       "41   -8.83       -0.56  Existing\n",
       "42   -9.42       -0.69  Existing\n",
       "44   -6.55       -1.27  Existing\n",
       "45   -4.08       -0.90  Existing\n",
       "46    2.86        2.32  Existing\n",
       "47    6.41        5.02  Existing\n",
       "48    2.09        1.44  Existing\n",
       "49    9.37        7.73  Existing"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict for isomap model\n",
    "predictions  ,df_all  ,df_suggestions,mean_abs_error= get_user_preds(model_isomap_deep, 19,df_user_jester_imputed,normalized_isomap_jester_user,True)\n",
    "print(\"mean_abs_error for this user\",round(mean_abs_error,2))\n",
    "df_all[df_all[\"status\"] == \"Existing\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.782"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_iso[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_iso shape: (2498300, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train_iso shape:\",x_train_iso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 54s - loss: 0.0277 - mean_absolute_error: 0.3102\n",
      "Epoch 2/10\n",
      " - 55s - loss: 0.0254 - mean_absolute_error: 0.3008\n",
      "Epoch 3/10\n",
      " - 44s - loss: 0.0245 - mean_absolute_error: 0.2963\n",
      "Epoch 4/10\n",
      " - 47s - loss: 0.0234 - mean_absolute_error: 0.2916\n",
      "Epoch 5/10\n",
      " - 52s - loss: 0.0222 - mean_absolute_error: 0.2860\n",
      "Epoch 6/10\n",
      " - 57s - loss: 0.0205 - mean_absolute_error: 0.2788\n",
      "Epoch 7/10\n",
      " - 55s - loss: 0.0181 - mean_absolute_error: 0.2682\n",
      "Epoch 8/10\n",
      " - 55s - loss: 0.0153 - mean_absolute_error: 0.2540\n",
      "Epoch 9/10\n",
      " - 52s - loss: 0.0129 - mean_absolute_error: 0.2403\n",
      "Epoch 10/10\n",
      " - 54s - loss: 0.0113 - mean_absolute_error: 0.2304\n",
      "for this fold absolute error 0.2121906280517578\n",
      "Epoch 1/10\n",
      " - 42s - loss: 0.0277 - mean_absolute_error: 0.3101\n",
      "Epoch 2/10\n",
      " - 41s - loss: 0.0255 - mean_absolute_error: 0.3012\n",
      "Epoch 3/10\n",
      " - 40s - loss: 0.0246 - mean_absolute_error: 0.2966\n",
      "Epoch 4/10\n",
      " - 40s - loss: 0.0235 - mean_absolute_error: 0.2920\n",
      "Epoch 5/10\n",
      " - 43s - loss: 0.0223 - mean_absolute_error: 0.2865\n",
      "Epoch 6/10\n",
      " - 43s - loss: 0.0207 - mean_absolute_error: 0.2799\n",
      "Epoch 7/10\n",
      " - 45s - loss: 0.0185 - mean_absolute_error: 0.2706\n",
      "Epoch 8/10\n",
      " - 43s - loss: 0.0159 - mean_absolute_error: 0.2579\n",
      "Epoch 9/10\n",
      " - 42s - loss: 0.0135 - mean_absolute_error: 0.2447\n",
      "Epoch 10/10\n",
      " - 42s - loss: 0.0116 - mean_absolute_error: 0.2330\n",
      "for this fold absolute error 0.212352454662323\n",
      "Epoch 1/10\n",
      " - 41s - loss: 0.0278 - mean_absolute_error: 0.3104\n",
      "Epoch 2/10\n",
      " - 40s - loss: 0.0255 - mean_absolute_error: 0.3010\n",
      "Epoch 3/10\n",
      " - 40s - loss: 0.0246 - mean_absolute_error: 0.2965\n",
      "Epoch 4/10\n",
      " - 42s - loss: 0.0235 - mean_absolute_error: 0.2914\n",
      "Epoch 5/10\n",
      " - 39s - loss: 0.0222 - mean_absolute_error: 0.2853\n",
      "Epoch 6/10\n",
      " - 40s - loss: 0.0205 - mean_absolute_error: 0.2782\n",
      "Epoch 7/10\n",
      " - 40s - loss: 0.0183 - mean_absolute_error: 0.2682\n",
      "Epoch 8/10\n",
      " - 39s - loss: 0.0157 - mean_absolute_error: 0.2553\n",
      "Epoch 9/10\n",
      " - 40s - loss: 0.0132 - mean_absolute_error: 0.2419\n",
      "Epoch 10/10\n",
      " - 39s - loss: 0.0116 - mean_absolute_error: 0.2316\n",
      "for this fold absolute error 0.2149825394153595\n",
      "Epoch 1/10\n",
      " - 39s - loss: 0.0277 - mean_absolute_error: 0.3105\n",
      "Epoch 2/10\n",
      " - 38s - loss: 0.0255 - mean_absolute_error: 0.3012\n",
      "Epoch 3/10\n",
      " - 38s - loss: 0.0246 - mean_absolute_error: 0.2968\n",
      "Epoch 4/10\n",
      " - 39s - loss: 0.0235 - mean_absolute_error: 0.2918\n",
      "Epoch 5/10\n",
      " - 38s - loss: 0.0222 - mean_absolute_error: 0.2861\n",
      "Epoch 6/10\n",
      " - 42s - loss: 0.0206 - mean_absolute_error: 0.2793\n",
      "Epoch 7/10\n",
      " - 47s - loss: 0.0184 - mean_absolute_error: 0.2696\n",
      "Epoch 8/10\n",
      " - 40s - loss: 0.0154 - mean_absolute_error: 0.2552\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "#as a last step make a kfold verification for our neural network\n",
    "#the results could not gurantee success on unseen data\n",
    "split_size = 8\n",
    "slicer = RepeatedKFold(n_splits=split_size, n_repeats=1, random_state=128452)\n",
    "all_scores = []\n",
    "for train_index, test_index in slicer.split(x_train_iso):\n",
    "\tX_train, X_test = x_train_iso[train_index], x_train_iso[test_index]\n",
    "\ty_train, y_test = y_train_iso[train_index], y_train_iso[test_index]\n",
    "\n",
    "\tmodel_isomap_deep = mlp_model_iso_deep()\n",
    "\tmodel_isomap_deep.fit(X_train,y_train,shuffle=True,\n",
    "                                    batch_size = 256,                                    \n",
    "                                    epochs = 10,\n",
    "                                    verbose = 2)\n",
    "\tscores = model_isomap_deep.evaluate(X_test, y_test, verbose=0)\n",
    "\tall_scores.append(scores[1])\n",
    "\tprint(\"for this fold absolute error\",scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold result\n",
    "#so kfold absolute error is also 0.2\n",
    "#since range is -1 to 1, it make %90 accuracy\n",
    "sum(all_scores) / len(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
